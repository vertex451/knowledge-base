## Requirements

### Functional Requirements

1. Create post - Upload photo or video with text
2. Follow user
3. Like post
4. Accounts can be public or private
5. Generate feed for user

### Non-functional Requirements

1. Availability
2. Persistency
3. Low latency (is 200ms for News Feed generation)


## Some Design Considerations

The system would be read-heavy, but users can upload as many photos as they like; therefore, efficient management of storage should be a crucial factor in designing this system.

## Capacity Estimation and Constraints

### Traffic estimates

- 1M daily active users, 500M total users
- user on average uploads 2 photos every day, so 2M posts/day

### Storage estimates

- Average photo size ~ 200kb
- Total storage for 1 day `2M * 200000b = 400B bytes = 400GB`
- Total storage for 10 years  `400GB * 365 * 10 = 1460000GB = 1460TB`

## High Level System Design

We need object storage to store photos and DB storage to store it's metadata

## Database Schema

Tables: Photo, User and Follows

A straightforward approach for storing the above schema would be to use an RDBMS like MySQL since we require joins. But relational databases come with their challenges, especially when we need to scale them.

## Data Size Estimation

**User**: UserID (4 bytes) + Name (20 bytes) + Email (32 bytes) + DateOfBirth (4 bytes) + CreationDate (4 bytes) + LastLogin (4 bytes) = 68 bytes

```
500 million * 68 ~= 32GB in total
```

**Photo:** PhotoID (4 bytes) + UserID (4 bytes) + PhotoPath (256 bytes) + PhotoLatitude (4 bytes) + PhotoLongitude(4 bytes) + UserLatitude (4 bytes) + UserLongitude (4 bytes) + CreationDate (4 bytes) = 284 bytes

```
2M * 284 bytes * 365 * 10years ~= 0.5GB per day * 365 * 10 = 1.88TB
```

**UserFollow:** Each row in the UserFollow table will consist of 8 bytes. If we have 500 million users and on average each user follows 500 users. 

```
500 million users * 500 followers * 8 bytes ~= 1.82TB
```

Total space required for all tables for 10 years will be 3.7TB:
```
32GB + 1.88TB + 1.82TB ~= 3.7TB
```

## Component Design

- Uploads goes to the disk and its a slow operation, while reads can be faster, if they are being served from cache.
- Uploads has higher priority, and it can consume all the available connections, and ‘reads’ cannot be served.
- If we assume that a web server can have a maximum of 500 connections at any time,  it would be wise to split read and writes to separate servises. It will also allow us to scale and optimize each of these operations independently.

## Reliability and Redundancy

Losing files is not an option for our service. Therefore, we will store multiple copies of each file. Also it will give us a hight availability and remove single point of failure.

If only one instance of a service is required to run at any point, we can run a redundant secondary copy of the service that is not serving any traffic, but it can take control after the failover when the primary has a problem.

## Data Sharding

### a. Partitioning based on UserID

Fo all tables we need to store 3.7TB of data in total. So, having shard for 1GB, we need 4 shards for it. Let’s assume, for better performance and scalability, we keep 10 shards.

We can find the shard number by **UserID % 10**

*If we remove from userID(UUID) dashes we will get hexadecimal number, which we can easily convert to decimal.*

To make user photos 

#### Issues:

1. Hot users, it will make difference in distribution over our shards.
2. What if the we run out of shard size? If we distribute photos of a user onto multiple shards, will it cause higher latencies?
3. Storing all photos of a user on one shard can cause issues like unavailability of all users data if that shard is down or higher latency if it is serving high load etc.

### b. Partitioning based on PhotoID





