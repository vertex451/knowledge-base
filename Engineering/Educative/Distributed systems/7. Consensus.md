# Problem
**Consensus problem** is common in DS - the various nodes of a distributed system try to reach an agreement on a specific thing:
- In the case of a distributed transaction, it’s whether a transaction has been committed or not.
- In the case of message delivery, it’s whether a message has been delivered or not.

_Consensus_ is the problem of making all nodes come to the agreement on something:
- value of X
- Leader election
- Distributed locking
- Atomic broadcast(concurrent broadcast messages while ensuring that all destinations consistently deliver them in the same sequence despite the possible presence of faulty nodes)
# FLP Impossibility
it’s impossible to develop a consensus algorithm that can always terminate successfully in asynchronous systems where at least one failure is possible:
- The fact that it’s always possible that the system’s initial state is one where nodes can reach different decisions depending on the ordering of messages (the so-called bivalent configuration), as long as there can be at least one faulty node
- From such a state, it’s always possible to end up in another bivalent state, just by introducing delays in some messages
# The Paxos Algorithm
2-phase commit protocol would have very limited fault tolerance, since the failure of a single node (the coordinator) could bring the whole system to a halt.

The obvious next step is to allow multiple nodes to inherit the role of the coordinator in these failure cases. This would then mean that there might be multiple masters that might produce conflicting results.

**Paxos algorithm** could solve the _consensus problem_ safely under these failures.

*This algorithm guarantees that the system will come to an agreement on a single value, and tolerate the failure of any number of nodes (potentially all of them), as long as more than half the nodes work properly at any time, which is a significant improvement.*

Funnily enough, this algorithm was invented by Leslie Lamport during his attempt to prove that this is actually impossible!

### Roles

The Paxos algorithm defines three different roles:
- Proposers
- Acceptors
- Learners
Every node in the system can potentially play multiple roles.
#### Proposer

A _proposer_ is responsible for proposing values (potentially received from clients’ requests) to the _acceptors_ and trying to persuade them to accept their value to arrive at a common decision.

#### Acceptor

An _acceptor_ is responsible for receiving these proposals and replying with their decision on whether this value can be chosen or not.

#### Learners

The _learners_ are responsible for learning the outcome of the consensus, storing it (in a replicated way) and potentially acting on it, by either notifying clients about the result or performing actions.

![[paxos.png]]

### Phases

The Paxos algorithm is split into two phases, each of which contains two parts:

#### Phase 1 (a)

A _proposer_ selects a number N and sends a prepare request with this number prepare(N) to at least a majority of the _acceptors_.

N is the **round Identifier**, which has two interesting properties:
1. The round identifier has to be bigger than any previous round identifier used by any other proposer in our Paxos cluster. This is achieved by incrementing a counter `i++`.
2. Make round identifiers unique because we never want two proposers to come up with the same round identifier and reuse it. Reusing identifiers would break the protocol. To achieve this we append `node_number` to the counter.

#### Phase 1 (b)

When receiving a prepare request, an _acceptor_ has the following options:

- If it has not already responded to another prepare request of a higher number N, it responds to the request with a promise not to accept any more proposals that are numbered less than N. It also returns the highest-numbered proposal it has accepted, if any (Note: the definition of a proposal follows).
- Otherwise, if it has already accepted a prepare request with a higher number, it rejects this prepare request. This ideally gives a hint to the proposer about the number of the other prepare request it has already responded to.

#### Phase 2 (a)

- If any of the _acceptors_ had already accepted another proposal and included that in its response, then the _proposer_ uses the value of the highest-numbered proposal among these responses. Essentially, this means that the _proposer_ attempts to bring the latest proposal to conclusion.
- Otherwise, if none of the _acceptors_ had accepted any other proposal, then the _proposer_ is free to select any desired value. This value is usually selected based on the clients’ requests.

#### Phase 2 (b)

If the _acceptor_ receives an accept(N, v) request for a proposal numbered N, it accepts the proposal, unless it has already responded to a prepare(k) request of a higher number (k>N).

# Handling edge cases

## Leader election problem

When a _proposer_ receives a response to a prepare message from a majority of nodes, it considers itself the (temporary) leader and proceeds with a proposal. 
If no other _proposer_ has attempted to become the leader in the meanwhile, its proposal will be accepted. 

However, if another _proposer_ managed to become a leader, the accept requests of the initial node will be rejected. This prevents multiple values to be chosen by the proposals of both nodes.

### Dueling proposers

The above solution results in a situation, where _proposers_ are continuously dueling each other, thus not making any progress

The most basic way is to force the _proposers_ to use random delays or exponential back-off every time they get their accept messages rejected and have to send a new prepare request. In this way, they give more time to the node that is currently leading to complete the protocol, by making a successful proposal, instead of competing.





