# Partitioning
Partitioning is the process of splitting a dataset into multiple, smaller datasets, with the following storing and processing them at different nodes. This allows us to increase the size of the data the system can handle by adding more nodes.

There are two different variations of partitioning:
1. Vertical partitioning
2. Horizontal partitioning (or **sharding**)

The terms “vertical” and “horizontal” originate from the era of relational databases.

**Vertical** partitioning involves splitting a table into multiple tables with fewer columns and using additional tables to store columns that relate rows across tables. We commonly refer to this as a **join operation**. We can then store these different tables in different nodes.

**Horizontal** partitioning, also known as sharding, involves splitting a table into multiple, smaller tables, where each table contains a percentage of the initial table’s rows. We can then store these different sub-tables in different nodes.
## Limitations of partitioning
- In a **vertically partitioned system**, requests that need to combine data from different tables (i.e., join operations) become less efficient. This is because these requests may now have to access data from multiple nodes.
- In a **horizontally partitioned system**,  we can usually avoid accessing data from multiple nodes because all the data for each row is located in the same node. However, we may still need to access data from multiple nodes for requests that are searching for a range of rows that belong to multiple nodes.

Another important implication of both vertical and horizontal partitioning is the potential for loss of transactional semantics: it’s much harder to perform atomic operations(where either _all_ or _none_ of them succeed) over data that resides in different nodes.
## Algorithms for Horizontal Partitioning
### Range partitioning
Is a technique where we split a dataset into ranges according to the value of a specific attribute.(first symbol of UUID for instance)

Is supported by PostgreSQL, MySQL, Snowflake, MongoDB, Cassandra, HBase, BigTable, DynamoDB, Redis and others.
#### Range query example in PostgreSQL:
We have orders data and we decided to split dataset by months, so each month is a partition.
And to get all orders from Jan 1st to March 31st we should run the following query:
```SQL
SELECT *
FROM sales_orders
WHERE order_date >= '2023-01-01' AND order_date <= '2023-03-31';
```
PostgreSQL's query planner and optimizer will use the partitioning information to perform partition pruning, which means it will only access and scan the partitions that are relevant to the specified date range. This optimization significantly reduces the amount of data that needs to be processed, improving query performance.

Partition metadata is stored in system catalogs, specifically in the `pg_partitioned_table` catalog table.
It contains information about the partitioning strategy (e.g., range, list, hash) and the partitioning key. 
#### Advantages
- Simplicity and ease of implementation.
- Makes adjusting ranges (re-partitioning) easier and more efficient. One range can be increased or decreased, which exchanges data only between two nodes.
- The ability to perform range queries using the partitioning key value.
- A good performance for range queries that use the partitioning key, when the queried range is small and resides in a single node. 
   In case of large dataset within one partition, we might need to create an index for a partition key.
#### Disadvantages
- An uneven distribution of the traffic or data, which causes some nodes to overload. 
- If you need to perform range query over regular columns, you need an additional index.
- A worse performance for range queries that use the partitioning key when the queried range is big and resides in multiple nodes
- If db engine itself doesn't natively provide built-in distributed partitioning across nodes(for instance, PostgreSQL), we might need to use additional tools to store the mapping.
### Hash partitioning
Is a technique where we apply a hash function which returns a number to a specific attribute of each row. We apply to this number Modulus(%) by partition number and the remainder will indicate which partition it will belong to:
```go
hash(value) % nodesNumber
```
#### Advantages of hash partitioning
- Better data distribution across the nodes
#### Disadvantages of hash partitioning
- Adding or removing nodes from the system causes it to re-partition(we need recalculated all keys). This results in significant data movement across all nodes of the system
- Poor range query performance - even if we have index for the column, we need to query all nodes.
### Consistent hashing
**Consistent hashing** is a partitioning technique that is very similar to hash partitioning, but solves the increased data movement problem caused by hash partitioning.

This is how it works: each node in the system is assigned a unique identifier, often derived from its network address or some other characteristic. This identifier is assigned in a range of `[0, L]`(is called ring (for example, `[0, 360]`)). Then, the system uses a record with an attribute value `s` as a partitioning key to locating the node **after the point** `hash(s) mod L` in the ring.

For example we have 3 nodes, which randomly took numbers:
```
node0 => 16
node1  => 103 
node3 => 187
node4 => 289
```
The result of `hash(s) mod L` will be assigned to the node which closes the range. 
![[consistent-hashing.png]]
In this case it is much easier to add new nodes.
#### Advantages of consistent hashing
-   Reduced data movement when nodes are added or removed in the system
#### Disadvantages of consistent hashing
- The potential for more imbalanced data distribution as nodes are added or removed. E.g., a node’s dataset is not distributed evenly across the system when it is removed but is instead transferred to a single node.
   We can mitigate these issues through the concept of “virtual nodes,” where we assign each physical node multiple locations in the ring.

Known systems - [[2. Databases#DynamoDB]] and [[2. Databases#Apache Cassandra]]
# Replication
Another benefit from using a distributed system is **availability** -  the ability of the system to remain functional despite failures in parts of it.

To achieve availability we use **replication** -  storing the same piece of data in multiple nodes (called replicas).

Replication can be:
- Pessimistic - tries to guarantee from the beginning that all the replicas are identical to each other in the any moment.
- Optimistic(or lazy replication) - allows the different replicas to diverge for some time, but eventually they will reach the same state.
## Single-Master Replication Algorithm
Is a technique where we designate a single node amongst the replicas as the **leader**, or primary, that receives all the updates and propagates it to the followers - other nodes which are used for read requests.
This technique is also known as **primary-backup replication**.
PostgreSQL and MySQL use a single-master replication technique that supports both asynchronous and synchronous replication.
### Techniques for propagating updates
#### Synchronous replication
The node replies to the client to indicate the update is complete— only after receiving acknowledgments from the other replicas that they’ve also performed the update on their local storage.

**Advantages** - increased **durability**. If leader crashes right after it acknowledges the update, all followers have the latest state. 

**Disadvantages** -  this technique can make writing requests slower, since the leader has to wait until it receives responses from all the replicas.
#### Asynchronous replication
The node replies to the client as soon as it performs the update in its local storage.
This technique increases performance significantly for write requests.
### Advantages of single-master replication
- Scalable for read-heavy workloads, just add more read replicas.
- Simplicity
- No need for complicated, distributed concurrency protocols, since concurrent operations are happening in the leader node.
- Easier to support transactional operations
### Disadvantages
- Not good for a write heavy workloads - the leader's capacity determines the capacity for writes
- It imposes an obvious trade-off between performance, durability, and consistency
- More followers lead to bottleneck in the network bandwidth of the leader node
- The process of failing over to a follower node when the leader node crashes, is not instant. This may create some downtime and also introduce the risk of errors
### How failover is being handled
### Manual approach
In the manual approach, the operator selects the new leader node and instructs all the nodes accordingly. This is the safest approach, but it incurs significant downtime and human resources.
#### Automated approach
This is faster but is quite risky. This is because there are many different ways in which the nodes can get confused and arrive at an incorrect state.
## Multi-Master Replication Algorithm
Technique that favours higher availability and performance over data consistency.
This technique is also known as **multi-primary replication**.
In this technique, all replicas are equal and can accept write requests. They are also responsible for propagating the data modifications to the rest of the group.
### Conflict resolution
Since write requests are concurrently handled by all the nodes, it can lead to the data inconsistency, if during the propagation of the first write request the client will send another one.
#### Exposing conflict resolution to the clients
An example of this is the shopping cart application, where the customer selects the correct version of their cart.
#### Last-write-wins conflict resolution
Each node in the system tags each version with a timestamp, using a local clock. During a conflict, the version with the latest timestamp is selected.
However, this technique can lead to some unexpected behaviors, as there is no global notion of time. 
#### Causality tracking algorithms
When there is a conflict between two writes (A, B) and one is determined to be the result of the other one, so the resulting write will be chosen.
## Quorums in Distributed Systems
The problem in synchronous replication is that availability is quite low for write operations, because the failure of a any single node makes the system unable to process writes until the node recovers.
If we take single master replication algorithm, it increases the availability of _writes_ significantly but decreases the availability of _reads_ at the same time. So, we have a trade-off that needs a mechanism to achieve a balance.
## Quorum
In a system of three replicas, we can say that writes need to complete in two nodes (as a quorum of two), while reads need to retrieve data from at least two nodes. This way, we can be sure that the reads will read the latest value. This is because at least one of the nodes in the _read quorum_ will also be included in the latest _write quorum_.

The system will work if it obey the following properties:
```
ReadReplicas + WriteReplicas > TotalReplicas
WriteReplicas > TotalReplicas/2

# In case of 3 replicas:
2R + 2W > 3
2W > 3/2

# In case of 5 replicas:
3R + 3W > 5
3W > 5/2
OR
2R + 4W > 5
4W > 5/2

# In case of 7 replicas
4R + 4W > 7
4W > 7/2
```
# Safety Guarantees in Distributed Systems
## Atomicity
Originate from ACID, it is challenging to achieve atomicity in DS because of the possibility of **partial failures**.
## Consistency 
Originate from CAP theorem, it is challenging to achieve consistency because of the **network asynchrony**.
## Isolation
Originate from ACID, it is challenging to achieve isolation because of the inherent **concurrency** of DS.
# ACID Transactions
**ACID** is a set of properties of traditional database transactions that provide guarantees around the expected behavior of transactions during errors, power failures, etc.
## Atomicity (A)
**Atomicity** guarantees that a transaction that consist of multiple operations is treated as a single unit. This means that either _all_ operations of the transaction are executed or _none_ of them.

This concept of atomicity extends to distributed systems, where the system might need to execute the same operation in multiple nodes of the system in an atomic way. So, the operation is either executed to all the nodes or none.
## Consistency (C)
**Consistency** guarantees that a transaction only transitions the database from one valid state to another valid state.
## Isolation (I)
**Isolation** guarantees that even though transactions might run concurrently and have data dependencies, they don't affect each other in unexpected way, and a result will be like they were executed successively.
## Durability (D)
**Durability** guarantees that once a transaction is committed, the new state remains persistent even in case of failure.
# The CAP Theorem
According to the initial statement of the CAP theorem, it is impossible for a **distributed data store** to provide all three properties simultaneously: **consistency**, **availability**, and **partition tolerance**.

But in case if the distributed system, it can be either _consistent_ or _available_ in the presence of a network partition.
### Consistency (C)
Consistency means that every successful read request receives the result of the most recent write request.
The meaning of consistency in the CAP theorem is more important for distributed systems than in ACID.
### Availability (A)
Availability means that every request receives a non-error response, but the response data may be stale.
### Partition Tolerance (P)
Partition tolerance means that the system will continue to operate in case of a **network partition**.

**In a distributed system, there is always the risk of a network partition. If this happens, the system needs to decide either to continue operating and compromise _data consistency_, or stop operating and compromise _availability_.**
## CAP theorem and known databases
CA is implemented in Microsoft SQL Server, Oracle Database, VoltDB
CP is implemented in Apache Cassandra, HBase, Google Cloud Bigtable
AP is implemented in Apache CouchDB, MongoDB, 
## CAP theorem in distributed systems
Distributed system can be either _consistent_ or _available_ in case of a network partition.
Example: we have two nodes and client sends write request `x = 10` and  read request `get x`.
Let's say that each request goes to different node and in case of the network partition write request is not propagated and the second node is not avare of new value of x.

**So, we should decide, do we want to give user stale data, or break the _availability_ property?**
But keep in mind, both _consistency_ and _availability_ properties can be satisfied when the network partition is not present.
## PACELC theorem
In the synchronous single-master replication system _consistency_ will be chosen over _latency_. 
Meanwhile, _asynchronous replication_ would reduce _latency_ at the cost of _consistency_.

It refers us to the ***PACELC theorem***:(Read by letters P A C + E L C)
“In the case of a _network partition_ (P), the system has to choose between _availability_ (A) and _consistency_ (C) but _else_ (E), when the system operates normally, the system has to choose between _latency_ (L) and _consistency_ (C).”

Based on this we can have 4 categories of DS:
- AP/EL
- CP/EL
- AP/EC
- CP/EC
The choice usually is between either low latency and availability, or data consistency. 
As a result, most of the systems fall into the AP/EL or CP/EC categories.
# Consistency Models
Consistency model defines the behaviours that are possible in a distributed system.
## Linearizability
Linearizability, also known as Atomic Consistency, guarantees that the outcome of concurrent operations appears as if they occurred one after the other in a linear order, even though multiple clients accessed and modified shared data concurrently.

This is achievable in DS with synchronous replication, but it causes bigger latency, since write operation waits for all nodes to receive the updated data.
#### Sequential consistency
Weaker consistency model that guarantee the linear order within single client, but doesn't require linear order between different clients.
For example, in a social networking application, we usually do not care what’s the ordering of posts between some of our friends. However, we still expect posts from a single friend to be displayed in the right order.
#### Causal consistency
In some cases, we don’t need to preserve the ordering specified by each client’s program—as long as causally related operations are displayed in the right order. This is the **causal consistency** model, which requires that only operations that are causally related need to be seen in the same order by all the nodes.

This model require some techniques like Tracking Causality, Vector Clocks, Causal Dependency Analysis

Causal Consistency strikes a balance between consistency and performance, but can introduce additional complexity compared to more strict consistency models.
#### Eventual consistency
where data may be temporarily inconsistent but will eventually become consistent over time as updates propagate through the system.

Example: Amazon - availability of product can be outdated, but when you add something to cart, Amazon does the check to ensure that product is in stock, and reserve it for you.
# CAP Theorem's Consistency Model
The “C” property in the CAP theorem refers to the linearizability model we previously described.
This means it’s impossible to build a system that will be _available_ during a network partition while also being _linearizable_.
### Strong consistency models
Strong consistency models correspond to the “C” in the CAP theorem and cannot be supported in systems that need to be _available_ during _network partitions_.
### Weak consistency models
Weak consistency models are the ones that can be supported while also preserving _availability_ during a network partition.
## Two commonly supported consistency models
Considering the guarantees provided by several popular distributed systems nowadays (i.e., Apache Cassandra, DynamoDB, etc.), two models are commonly supported.
- Strong consistency, specifically _linearizability_
Since we can give up availability, it seems reasonable to provide the strongest model amongst the available ones
- Weak consistency, specifically _eventual consistency_
Was selected amongst the available weak consistency models thanks to its simplicity and performance
# Isolation Levels and Anomalies
The inherent **concurrency** in distributed systems creates the potential for **anomalies** and unexpected behaviours. Specifically, transactions that comprise multiple operations and run concurrently can lead to different results depending on how their operations are interleaved.

**isolation levels** formal models that define what is possible and what is not in a concurrent system’s behaviour.

The most common isolation levels:
- **Serializability:** It essentially states that two transactions, when executed concurrently, should give the same result as though executed sequentially.
- **Repeatable read:** It ensures that the data once read by a transaction will not change throughout its course.
- **Snapshot isolation:** It guarantees that all reads made in a transaction see a consistent snapshot of the database from the point it started and till the transaction commits successfully if no other transaction has updated the same data since that snapshot.
- **Read committed:** It does not allow transactions to read data that has not yet been committed by another transaction.
- **Read uncommitted:** It is the lowest isolation level and allows the transaction to read uncommitted data by other transactions.
Of course, stronger isolation levels prevent more anomalies at the cost of performance.
## Anomalies
- Dirty writes
- Dirty reads
- (Fuzzy) non-repeatable reads
- Phantom reads
- Lost updates
### Dirty write
A **dirty write** occurs when a transaction overwrites a value that was previously written by another transaction that is still in-flight and has not been committed yet.
Another problem with dirty writes is they make it impossible for the system to automatically rollback to a previous image of the database. As a result, this is an anomaly we need to prevent in most cases.
### Dirty read
A **dirty read** occurs when a transaction reads a value that has been written by another transaction that has not yet been committed.
This is problematic since the initial transactions might be rolled back.
But dirty read can be useful when we troubleshoot an issue and want to inspect the state of the database in the middle of an ongoing transaction.
### Fuzzy or non-repeatable read
A **fuzzy or non-repeatable read** occurs when a value is retrieved twice during a transaction (without it being updated in the same transaction), and the value is different.
### Phantom read
For example, transaction A runs 2 queries to calculate the maximum and the average age of a specific set of employees. However, between the two queries, transaction B is interleaved and inserts a lot of old employees in this set, thus making transaction A return an average that is larger than the maximum.
### Lost update
Transaction A and Transaction B read value of x and tries to add 2 to it. Initially X = 10, and in the consequent way it should end up with the value 14, but if both transactions read 10, each will write 12.
# Prevention of Anomalies in Isolation Levels
There is one isolation level that prevents all of these anomalies: the **serializable** one.
However, serializability has performance costs since it intentionally reduces concurrency to guarantee safety.
# Consistency and Isolation
_Consistency models_ are applied to single-object operations (e.g. read/write to a specific field), while _isolation levels_ are applied to multi-object operations (e.g. read and write from/to multiple rows in a table within a transaction).
- _Linearizability_ guarantees that the effects of an operation took place at some point between when the client invoked the operation, and when the result of the operation was returned to the client.
- _Serializability_ only guarantees that the effects of multiple transactions will be the same as if they run in serial order. It does not provide any guarantee on whether that serial order would be compatible with real-time order.
# Hierarchy of Models
Hierarchy of consistency levels and isolation models, where the strictness level decreases from top to bottom:
![[consistency-isolation-levels.png]]































 







